import os
from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.runnables import RunnableParallel, RunnableLambda
from langchain.prompts import ChatPromptTemplate
from .contact_info import get_district_contact, get_contact_info_text
from .useful_links import get_relevant_links  # â† ìˆ˜ì •: get_related_links â†’ get_relevant_links


# â­ .env íŒŒì¼ ë¡œë“œ
load_dotenv()


# --- í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ ---
SYSTEM_TEMPLATE = """
ë‹¹ì‹ ì€ ì „ì„¸ì‚¬ê¸° í”¼í•´ì ì§€ì›ì„¼í„°ì˜ ì¹œì ˆí•œ ìƒë‹´ì›ì…ë‹ˆë‹¤.


## ë‹µë³€ í˜•ì‹ (3ê°œ ì„¹ì…˜ë§Œ)


### ğŸ¯ 1. ê³ ê°ë‹˜ì˜ ìƒí™©
- ì§„ë‹¨ ê²°ê³¼ë¥¼ 1-2ë¬¸ì¥ìœ¼ë¡œ ê°„ë‹¨íˆ ì„¤ëª…
- ë°›ìœ¼ì‹¤ ìˆ˜ ìˆëŠ” ì§€ì› ë“±ê¸‰ ëª…ì‹œ


### ğŸ’° 2. ë°›ìœ¼ì‹¤ ìˆ˜ ìˆëŠ” ì§€ì›
**êµ¬ì²´ì ì¸ ê¸ˆì•¡, ê¸ˆë¦¬, í•œë„ë¥¼ <context>ì—ì„œ ì°¾ì•„ ë°˜ë“œì‹œ í¬í•¨**:


#### ğŸ  ì£¼ê±° ì§€ì›
- ê¸´ê¸‰ ì£¼ê±°ë¹„, ê³µê³µì„ëŒ€ì£¼íƒ ë“± (ê¸ˆì•¡ ëª…ì‹œ)


#### ğŸ’³ ê¸ˆìœµ ì§€ì›
- ëŒ€ì¶œ ìƒí’ˆë³„ ê¸ˆë¦¬, í•œë„ ëª…ì‹œ
- ì˜ˆ: ë””ë”¤ëŒ ëŒ€ì¶œ ì—° 1.85~2.70%, ìµœëŒ€ 3ì–µì›


#### ğŸ“‹ ê¸°íƒ€ ì§€ì›
- ìƒê³„ë¹„, ë²•ë¥ ì§€ì› ë“±


### ğŸ“ 3. ì‹ ì²­ ë°©ë²• ë° ì„œë¥˜


**ë‹¨ê³„ë³„ë¡œ ì„¤ëª…**:
1ï¸âƒ£ í•„ìš”í•œ ì„œë¥˜ (ì²´í¬ë¦¬ìŠ¤íŠ¸ í˜•ì‹)
2ï¸âƒ£ ì‹ ì²­ ì ˆì°¨ (ê°„ë‹¨íˆ 3ë‹¨ê³„ ì´ë‚´)


## ë§íˆ¬ ê·œì¹™
- ì§§ê³  ëª…í™•í•œ ë¬¸ì¥ (í•œ ë¬¸ì¥ 2ì¤„ ì´ë‚´)
- ë”°ëœ»í•œ ì¡´ëŒ“ë§
- ì´ëª¨ì§€ë¡œ ê°€ë…ì„± í–¥ìƒ
- ë²ˆí˜¸ì™€ ì²´í¬ë°•ìŠ¤ë¡œ êµ¬ì¡°í™”


<context>
{context}
</context>
"""


RAG_PROMPT = ChatPromptTemplate.from_messages([
    ("system", SYSTEM_TEMPLATE),
    ("human", "ì‚¬ìš©ì ìƒí™©: {user_situation}\n\nì§ˆë¬¸: {user_query}"),
])


# í˜„ì¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì„¤ì •
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
DB_PATH = os.path.join(CURRENT_DIR, "index")
DB_NAME = "jeonse_vector_index"
MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"


# â­ í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")


if not GOOGLE_API_KEY:
    raise ValueError("âŒ GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")


# ----------------------------------------------------
# RAG ì²´ì¸ êµ¬ì„± í•¨ìˆ˜
# ----------------------------------------------------


def create_rag_chain():
    """ì»¤ìŠ¤í…€ RAG ì²´ì¸ ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    print("  -> ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
    embeddings = HuggingFaceEmbeddings(model_name=MODEL_NAME)
    
    print("  -> FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì¤‘...")
    vectorstore = FAISS.load_local(
        folder_path=DB_PATH, 
        index_name=DB_NAME, 
        embeddings=embeddings, 
        allow_dangerous_deserialization=True
    )
    
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

    print("  -> Google Gemini API ì—°ê²° ì¤‘...")
    llm = ChatGoogleGenerativeAI(
        model="models/gemini-2.5-flash",
        temperature=0.2,
        google_api_key=GOOGLE_API_KEY
    )
    
    def format_docs(docs):
        """ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©í•©ë‹ˆë‹¤."""
        return "\n\n".join(doc.page_content for doc in docs)
    
    rag_chain = (
        RunnableParallel({
            "context": RunnableLambda(
                lambda x: format_docs(
                    retriever.invoke(f"{x['user_situation']} {x['user_query']}")
                )
            ),
            "user_situation": RunnableLambda(lambda x: x["user_situation"]),
            "user_query": RunnableLambda(lambda x: x["user_query"])
        })
        | RAG_PROMPT
        | llm
    )
    
    print("  -> RAG ì²´ì¸ ìƒì„± ì™„ë£Œ")
    return rag_chain


# ----------------------------------------------------
# í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ (ê´€ë ¨ ë§í¬ìš©)
# ----------------------------------------------------


def extract_keywords_from_query(query: str) -> list:
    """
    ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    
    Args:
        query: ì‚¬ìš©ì ì§ˆë¬¸
        
    Returns:
        ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    """
    keyword_map = {
        "ì£¼ê±°": ["ì£¼ê±°", "ì§‘", "ì„ëŒ€", "ì „ì„¸", "ê¸´ê¸‰ì£¼ê±°ë¹„", "ê³µê³µì„ëŒ€"],
        "ê¸ˆìœµ": ["ê¸ˆìœµ", "ëŒ€ì¶œ", "ì´ì", "ìƒí™˜", "ë””ë”¤ëŒ", "ë²„íŒ€ëª©", "ê¸ˆë¦¬"],
        "ë²•ë¥ ": ["ë²•ë¥ ", "ë³€í˜¸ì‚¬", "ì†Œì†¡", "ê²½ë§¤", "ëŒ€í•­ë ¥"],
        "ìƒê³„": ["ìƒê³„", "ìƒí™œë¹„", "ê¸´ê¸‰", "ë³µì§€"],
        "ì‹ ì²­": ["ì‹ ì²­", "ì ˆì°¨", "ì„œë¥˜", "ë°©ë²•"],
    }
    
    keywords = []
    for category, words in keyword_map.items():
        if any(word in query for word in words):
            keywords.append(category)
    
    return keywords


# ----------------------------------------------------
# RAG ì‘ë‹µ ìƒì„± í•¨ìˆ˜
# ----------------------------------------------------


def get_rag_response(user_situation: str, user_query: str, district: str = None) -> str:
    """
    AI ë‹´ë‹¹ 2ì—ì„œ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” ì¸í„°í˜ì´ìŠ¤ í•¨ìˆ˜
    
    Args:
        user_situation: AI ë‹´ë‹¹ 2ê°€ íŒë³„í•œ ìƒí™©
        user_query: ì‚¬ìš©ìì˜ ì§ˆë¬¸
        district: ì‚¬ìš©ìì˜ ê±°ì£¼ ìì¹˜êµ¬ (ì„ íƒì‚¬í•­)
    
    Returns:
        AI ë‹´ë‹¹ 1ì˜ ë‹µë³€ (ë¬¸ìì—´)
    """
    faiss_file_path = os.path.join(DB_PATH, f"{DB_NAME}.faiss")
    if not os.path.exists(faiss_file_path):
        return f"ğŸš¨ ì˜¤ë¥˜: ë²¡í„° DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\nê²½ë¡œ: {faiss_file_path}"
    
    rag_chain = create_rag_chain()
    
    try:
        response = rag_chain.invoke({
            "user_situation": user_situation,
            "user_query": user_query
        })
        
        # ê¸°ë³¸ ë‹µë³€
        answer = response.content
        
        # â­ ê´€ë ¨ ë§í¬ ì¶”ê°€ (í‚¤ì›Œë“œ ê¸°ë°˜)
        keywords = extract_keywords_from_query(user_query)
        if keywords:
            related_links = get_relevant_links(keywords)  # â† ìˆ˜ì •
            if related_links:
                answer += "\n\n" + "="*70
                answer += "\nğŸ”— ê´€ë ¨ ìœ ìš©í•œ ë§í¬\n"
                answer += "="*70 + "\n"
                answer += related_links
        
        # ìì¹˜êµ¬ ì •ë³´ê°€ ìˆìœ¼ë©´ ì—°ë½ì²˜ ì¶”ê°€
        if district:
            contact_info = get_contact_info_text(district)  # â† ìˆ˜ì •: í†µì¼ëœ í•¨ìˆ˜ëª… ì‚¬ìš©
            if contact_info:
                answer += "\n\n" + "="*70
                answer += f"\nğŸ“ {district} ì—°ë½ì²˜\n"
                answer += "="*70 + "\n"
                answer += contact_info
        
        return answer
        
    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        return f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\n\nìƒì„¸:\n{error_detail}"


# ----------------------------------------------------
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)
# ----------------------------------------------------


if __name__ == "__main__":
    
    faiss_file_path = os.path.join(DB_PATH, f"{DB_NAME}.faiss")
    if not os.path.exists(faiss_file_path):
        print(f"ğŸš¨ ì˜¤ë¥˜: ë²¡í„° DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\nê²½ë¡œ: {faiss_file_path}")
    else:
        print("\n" + "="*60)
        print("ğŸš€ RAG ì²´ì¸ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("ğŸ’¡ ì‚¬ìš© ëª¨ë¸: Google Gemini 2.5 Flash (API)")
        print("="*60 + "\n")
        
        # í…ŒìŠ¤íŠ¸ ì…ë ¥
        test_input = {
            "user_situation": "í”¼í•´ì ê²°ì • (ëª¨ë“  ì§€ì› ê°€ëŠ¥). ê²½ë§¤ ì§„í–‰ ì¤‘. ì¢…ë¡œêµ¬ ê±°ì£¼.",
            "user_query": "ì§€ê¸ˆ ë‹¹ì¥ í•´ì•¼ í•  3ê°€ì§€ ì¡°ì¹˜ì™€ ê²½ë§¤ë¡œ ì§‘ì„ ëºê¸°ì§€ ì•Šê³  ê³„ì† ì‚´ ë°©ë²•ì´ ê¶ê¸ˆí•©ë‹ˆë‹¤."
        }
        
        print("\nğŸ’¬ Google Gemini APIë¥¼ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...\n")
        
        response = get_rag_response(
            test_input["user_situation"], 
            test_input["user_query"],
            district="ì¢…ë¡œêµ¬"  # â† í…ŒìŠ¤íŠ¸ì— ìì¹˜êµ¬ ì¶”ê°€
        )
        
        print("\n" + "="*60)
        print(f"ğŸ“ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸: {test_input['user_query']}")
        print("="*60 + "\n")
        print(response)
        print("\n" + "="*60)
        print("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ!")
        print("="*60 + "\n")
