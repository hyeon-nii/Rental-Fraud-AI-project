import os
from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.runnables import RunnableParallel, RunnableLambda
from langchain.prompts import ChatPromptTemplate

# â­ .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# --- í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ ---
SYSTEM_TEMPLATE = """
ë‹¹ì‹ ì€ ì „ì„¸ì‚¬ê¸° í”¼í•´ì ì§€ì›ì„¼í„°ì˜ ìƒë‹´ì›ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì—ê²Œ ê³µê°í•˜ê³ , ì¹¨ì°©í•˜ë©°, ì§€ì› ì •ë³´ë¥¼ ëª…í™•í•˜ê²Œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.

1. ì‚¬ìš©ìì˜ ìƒí™©ê³¼ ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ì •ë³´ë¥¼ <context>ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
2. ë‹µë³€ì€ ë°˜ë“œì‹œ ë‹¤ìŒ 4ê°€ì§€ ì„¹ì…˜ì„ H2 ë§ˆí¬ë‹¤ìš´ í—¤ë”©ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.
   - ## 1. ì‚¬ìš©ì ìƒí™© íŒë‹¨ ë° ì§€ì› ë“±ê¸‰
   - ## 2. ì§€ì› ê°€ëŠ¥í•œ í˜œíƒ ëª©ë¡
   - ## 3. ì‹ ì²­ ì¡°ê±´ ë° ì œì¶œ ì„œë¥˜
   - ## 4. ê´€í•  ìì¹˜êµ¬ ì—°ë½ì²˜
3. ì§€ì› í˜œíƒ ëª©ë¡ì€ <context>ì˜ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë””ë”¤ëŒ ê¸ˆë¦¬, ëŒ€ì¶œ í•œë„ ë“± êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
4. ì‚¬ìš©ìê°€ 'ì£½ê³ ì‹¶ë‹¤', 'í¬ê¸°' ë“± ìœ„ê¸° í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ë©´, ë‹¤ë¥¸ ëª¨ë“  ì •ë³´ë³´ë‹¤ 1393 ìì‚´ ì˜ˆë°© ìƒë‹´ ì „í™” ë²ˆí˜¸ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ì•ˆë‚´í•´ì•¼ í•©ë‹ˆë‹¤.

<context>
{context}
</context>
"""

RAG_PROMPT = ChatPromptTemplate.from_messages([
    ("system", SYSTEM_TEMPLATE),
    ("human", "ì‚¬ìš©ì ìƒí™©: {user_situation}\n\nì§ˆë¬¸: {user_query}"),
])

# í˜„ì¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì„¤ì •
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
DB_PATH = os.path.join(CURRENT_DIR, "index")
DB_NAME = "jeonse_vector_index"
MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"

# â­ í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

if not GOOGLE_API_KEY:
    raise ValueError("âŒ GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")


# ----------------------------------------------------
# RAG ì²´ì¸ êµ¬ì„± í•¨ìˆ˜
# ----------------------------------------------------

def create_rag_chain():
    """ì»¤ìŠ¤í…€ RAG ì²´ì¸ ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    print("  -> ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
    embeddings = HuggingFaceEmbeddings(model_name=MODEL_NAME)
    
    print("  -> FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì¤‘...")
    vectorstore = FAISS.load_local(
        folder_path=DB_PATH, 
        index_name=DB_NAME, 
        embeddings=embeddings, 
        allow_dangerous_deserialization=True
    )
    
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

    print("  -> Google Gemini API ì—°ê²° ì¤‘...")
    llm = ChatGoogleGenerativeAI(
        model="models/gemini-2.5-flash",
        temperature=0.2,
        google_api_key=GOOGLE_API_KEY
    )
    
    def format_docs(docs):
        """ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©í•©ë‹ˆë‹¤."""
        return "\n\n".join(doc.page_content for doc in docs)
    
    rag_chain = (
        RunnableParallel({
            "context": RunnableLambda(
                lambda x: format_docs(
                    retriever.invoke(f"{x['user_situation']} {x['user_query']}")
                )
            ),
            "user_situation": RunnableLambda(lambda x: x["user_situation"]),
            "user_query": RunnableLambda(lambda x: x["user_query"])
        })
        | RAG_PROMPT
        | llm
    )
    
    print("  -> RAG ì²´ì¸ ìƒì„± ì™„ë£Œ")
    return rag_chain


# â­ AI ë‹´ë‹¹ 2ì—ì„œ í˜¸ì¶œí•  ìˆ˜ ìˆë„ë¡ í•¨ìˆ˜ë¡œ ë¶„ë¦¬
def get_rag_response(user_situation: str, user_query: str) -> str:
    """
    AI ë‹´ë‹¹ 2ì—ì„œ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” ì¸í„°í˜ì´ìŠ¤ í•¨ìˆ˜
    
    Args:
        user_situation: AI ë‹´ë‹¹ 2ê°€ íŒë³„í•œ ìƒí™© (ì˜ˆ: "í”¼í•´ì ê²°ì • (ëª¨ë“  ì§€ì› ê°€ëŠ¥)")
        user_query: ì‚¬ìš©ìì˜ ì§ˆë¬¸
    
    Returns:
        AI ë‹´ë‹¹ 1ì˜ ë‹µë³€ (ë¬¸ìì—´)
    """
    if not os.path.exists(f"{DB_PATH}/{DB_NAME}.faiss"):
        return "ğŸš¨ ì˜¤ë¥˜: ë²¡í„° DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."
    
    rag_chain = create_rag_chain()
    
    try:
        response = rag_chain.invoke({
            "user_situation": user_situation,
            "user_query": user_query
        })
        return response.content
    except Exception as e:
        return f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}"


# ----------------------------------------------------
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)
# ----------------------------------------------------

if __name__ == "__main__":
    
    if not os.path.exists(f"{DB_PATH}/{DB_NAME}.faiss"):
        print("ğŸš¨ ì˜¤ë¥˜: ë²¡í„° DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("\n" + "="*60)
        print("ğŸš€ RAG ì²´ì¸ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("ğŸ’¡ ì‚¬ìš© ëª¨ë¸: Google Gemini 2.5 Flash (API)")
        print("="*60 + "\n")
        
        # í…ŒìŠ¤íŠ¸ ì…ë ¥
        test_input = {
            "user_situation": "í”¼í•´ì ê²°ì • (ëª¨ë“  ì§€ì› ê°€ëŠ¥). ê²½ë§¤ ì§„í–‰ ì¤‘. ì¢…ë¡œêµ¬ ê±°ì£¼.",
            "user_query": "ì§€ê¸ˆ ë‹¹ì¥ í•´ì•¼ í•  3ê°€ì§€ ì¡°ì¹˜ì™€ ê²½ë§¤ë¡œ ì§‘ì„ ëºê¸°ì§€ ì•Šê³  ê³„ì† ì‚´ ë°©ë²•ì´ ê¶ê¸ˆí•©ë‹ˆë‹¤."
        }
        
        print("\nğŸ’¬ Google Gemini APIë¥¼ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...\n")
        
        response = get_rag_response(
            test_input["user_situation"], 
            test_input["user_query"]
        )
        
        print("\n" + "="*60)
        print(f"ğŸ“ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸: {test_input['user_query']}")
        print("="*60 + "\n")
        print(response)
        print("\n" + "="*60)
        print("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ!")
        print("="*60 + "\n")
